{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Telegram Meteora Pool Analyzer Report Generator\n",
    "\n",
    "## ğŸ“Š Overview\n",
    "This notebook generates automated liquidity pool analysis reports using Gateway's Meteora CLMM endpoints and distributes them via Telegram. It fetches real-time pool data, analyzes liquidity distribution, price movements, and generates comprehensive reports with charts.\n",
    "\n",
    "## ğŸ“‹ Prerequisites\n",
    "- Gateway service running locally (default: http://localhost:15888)\n",
    "- Telegram bot configuration (bot token and chat ID in .env)\n",
    "\n",
    "## ğŸ“ˆ Expected Outputs\n",
    "- Real-time pool metrics and liquidity analysis\n",
    "- Liquidity distribution charts showing bin concentrations\n",
    "- CSV reports with detailed bin data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”Œ Initialize Core Services and Libraries\n",
    "\n",
    "## âš™ï¸ Configuration and Gateway API Client\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from dotenv import load_dotenv\n",
    "from typing import Dict, List, Optional, Any\n",
    "import json\n",
    "\n",
    "# Add the project root to the path to ensure imports work\n",
    "project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath('__file__'))))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# Import QuantsLab Gateway Data Source\n",
    "from core.data_sources.gateway import GatewayDataSource\n",
    "\n",
    "# Import QuantsLab notification system\n",
    "from core.notifiers import NotificationManager, NotificationMessage\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"âœ… Libraries imported successfully\")\n",
    "print(\"ğŸ“Š Services available: GatewayDataSource (QuantsLab), Notification system\")\n",
    "print(\"âš™ï¸ Environment variables loaded from .env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Parameters and Gateway Setup\n",
    "GATEWAY_URL = os.getenv('GATEWAY_URL', 'http://localhost:15888')\n",
    "NETWORK = 'mainnet-beta'\n",
    "UPDATE_INTERVAL_SECONDS = 300  # 5 minutes between updates\n",
    "\n",
    "# Pool addresses to analyze - add your pool addresses here\n",
    "POOL_ADDRESSES = [\n",
    "    '2sf5NYcY4zUPXUSmG6f66mskb24t5F8S11pC1Nz5nQT3',  # SOL-USDC\n",
    "    'C8Gr6AUuq9hEdSYJzoEpNcdjpojPZwqG5MtQbeouNNwg',  # JUP-SOL\n",
    "    '9d9mb8kooFfaD3SctgZtkxQypkshx6ezhbKio89ixyy2',  # TRUMP-USDC\n",
    "    # Add more pool addresses here as needed\n",
    "    # 'your_pool_address_here',\n",
    "]\n",
    "\n",
    "# Initialize Gateway Data Source\n",
    "gateway_ds = GatewayDataSource(gateway_url=GATEWAY_URL, network=NETWORK)\n",
    "\n",
    "# Initialize notification manager\n",
    "notification_manager = NotificationManager()\n",
    "enabled_notifiers = notification_manager.get_enabled_notifiers()\n",
    "\n",
    "# Initialize collections for Telegram sending\n",
    "telegram_reports = {\n",
    "    'text_reports': [],\n",
    "    'chart_files': [],\n",
    "    'csv_files': [],\n",
    "    'report_message': None\n",
    "}\n",
    "\n",
    "# ğŸ“Š Meteora Pool Analyzer Class\n",
    "class MeteorPoolAnalyzer:\n",
    "    \"\"\"Helper class for fetching and analyzing Meteora pools using GatewayDataSource\"\"\"\n",
    "    \n",
    "    def __init__(self, gateway_ds: GatewayDataSource):\n",
    "        self.gateway_ds = gateway_ds\n",
    "        self.client = gateway_ds.client\n",
    "        \n",
    "    async def fetch_pool_info(self, pool_address: str, network: str = 'mainnet-beta') -> Optional[Dict]:\n",
    "        \"\"\"Fetch Meteora pool information from Gateway\"\"\"\n",
    "        try:\n",
    "            # Make request to Gateway's Meteora endpoint\n",
    "            response = await self.client.api_request(\n",
    "                method=\"get\",\n",
    "                path_url=\"connectors/meteora/clmm/pool-info\",\n",
    "                params={\n",
    "                    'network': network,\n",
    "                    'poolAddress': pool_address\n",
    "                }\n",
    "            )\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error fetching pool info: {e}\")\n",
    "            return None\n",
    "    \n",
    "    async def fetch_token_info(self, token_address: str, network: str = 'mainnet-beta') -> Optional[Dict]:\n",
    "        \"\"\"Fetch token information from Gateway with caching\"\"\"\n",
    "        return await self.gateway_ds.fetch_token_info(token_address, network)\n",
    "    \n",
    "    def calculate_liquidity_distribution(self, bins_df, current_price: float, total_liquidity: float) -> Dict[str, float]:\n",
    "        \"\"\"Calculate liquidity distribution at different price ranges\"\"\"\n",
    "        return self.gateway_ds.calculate_price_range_distribution(\n",
    "            bins_df, current_price, 'price', 'total_value'\n",
    "        )\n",
    "    \n",
    "    def analyze_pool(self, pool_data: Dict, metadata: Dict = None) -> Dict:\n",
    "        \"\"\"Analyze pool data and calculate metrics\"\"\"\n",
    "        # Add metadata if provided\n",
    "        if metadata:\n",
    "            pool_data.update(metadata)\n",
    "            \n",
    "        # Calculate total liquidity\n",
    "        price = pool_data['price']\n",
    "        base_amount = pool_data['baseTokenAmount']\n",
    "        quote_amount = pool_data['quoteTokenAmount']\n",
    "        total_liquidity_usd = quote_amount + (base_amount * price)\n",
    "        \n",
    "        analysis = {\n",
    "            'address': pool_data['address'],\n",
    "            'pool_name': pool_data.get('pool_name', 'Unknown'),\n",
    "            'base_token': pool_data.get('base_token_symbol', pool_data.get('base_token', 'BASE')),\n",
    "            'quote_token': pool_data.get('quote_token_symbol', pool_data.get('quote_token', 'QUOTE')),\n",
    "            'base_token_address': pool_data.get('baseTokenAddress'),\n",
    "            'quote_token_address': pool_data.get('quoteTokenAddress'),\n",
    "            'current_price': price,\n",
    "            'fee_pct': pool_data['feePct'],\n",
    "            'dynamic_fee_pct': pool_data.get('dynamicFeePct', pool_data['feePct']),\n",
    "            'bin_step': pool_data.get('binStep', 0),\n",
    "            'active_bin_id': pool_data.get('activeBinId'),\n",
    "            'base_liquidity': base_amount,\n",
    "            'quote_liquidity': quote_amount,\n",
    "            'total_liquidity_usd': total_liquidity_usd,\n",
    "            'num_bins': len(pool_data.get('bins', [])),\n",
    "            'liquidity_concentration': 0,\n",
    "            'liquidity_distribution': {},\n",
    "            'price_range': {'min': 0, 'max': 0},\n",
    "            'timestamp': datetime.now(timezone.utc)\n",
    "        }\n",
    "        \n",
    "        # Analyze bins if available\n",
    "        bins = pool_data.get('bins', [])\n",
    "        if bins:\n",
    "            bins_df = pd.DataFrame(bins)\n",
    "            \n",
    "            # Calculate price range\n",
    "            analysis['price_range']['min'] = bins_df['price'].min()\n",
    "            analysis['price_range']['max'] = bins_df['price'].max()\n",
    "            \n",
    "            # Calculate bin values\n",
    "            bins_df['total_value'] = (\n",
    "                bins_df['quoteTokenAmount'] + \n",
    "                bins_df['baseTokenAmount'] * price\n",
    "            )\n",
    "            \n",
    "            total_liq = bins_df['total_value'].sum()\n",
    "            \n",
    "            # Calculate concentration around active bin\n",
    "            if analysis['active_bin_id'] is not None:\n",
    "                active_bin_id = analysis['active_bin_id']\n",
    "                nearby_bins = bins_df[\n",
    "                    (bins_df['binId'] >= active_bin_id - 5) &\n",
    "                    (bins_df['binId'] <= active_bin_id + 5)\n",
    "                ]\n",
    "                \n",
    "                nearby_liq = nearby_bins['total_value'].sum() if not nearby_bins.empty else 0\n",
    "                \n",
    "                if total_liq > 0:\n",
    "                    analysis['liquidity_concentration'] = (nearby_liq / total_liq) * 100\n",
    "            \n",
    "            # Calculate liquidity distribution at different price ranges using GatewayDataSource\n",
    "            analysis['liquidity_distribution'] = self.calculate_liquidity_distribution(\n",
    "                bins_df, price, total_liq\n",
    "            )\n",
    "            \n",
    "            # Find key liquidity levels\n",
    "            top_bins = bins_df.nlargest(5, 'total_value')\n",
    "            analysis['key_levels'] = [\n",
    "                {'price': row['price'], 'total_value': row['total_value']}\n",
    "                for _, row in top_bins.iterrows()\n",
    "            ]\n",
    "            \n",
    "            analysis['bins_df'] = bins_df\n",
    "            \n",
    "        return analysis\n",
    "\n",
    "# Initialize the analyzer\n",
    "pool_analyzer = MeteorPoolAnalyzer(gateway_ds)\n",
    "\n",
    "async def generate_pool_report(analyses: List[Dict]) -> str:\n",
    "    \"\"\"Generate formatted pool analysis report for Telegram using cached token symbols\"\"\"\n",
    "    \n",
    "    if not analyses:\n",
    "        return \"âš ï¸ No pool data available for analysis\"\n",
    "    \n",
    "    # Generate report timestamp\n",
    "    report_time = gateway_ds.format_timestamp(datetime.now(timezone.utc))\n",
    "    \n",
    "    # Build the report message\n",
    "    report_message = f\"\"\"ğŸŠ <b>Meteora Pool Analysis Report</b>\n",
    "ğŸ“… {report_time}\n",
    "ğŸ”— Network: Solana {NETWORK}\n",
    "âš¡ Powered by Gateway (via QuantsLab GatewayDataSource)\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    # Add analysis for each pool\n",
    "    for analysis in analyses:\n",
    "        # Use cached token symbols from initial pool fetching\n",
    "        base_symbol = analysis['base_token']\n",
    "        quote_symbol = analysis['quote_token']\n",
    "        \n",
    "        # Only try to fetch if we don't already have symbols and they're not addresses\n",
    "        if analysis.get('base_token_address') and len(base_symbol) > 10:  # Likely an address, not symbol\n",
    "            cache_key = f\"{analysis['base_token_address']}_{NETWORK}\"\n",
    "            if cache_key in gateway_ds.token_cache:\n",
    "                cached_info = gateway_ds.token_cache[cache_key]\n",
    "                if cached_info and 'symbol' in cached_info:\n",
    "                    base_symbol = cached_info['symbol']\n",
    "                    \n",
    "        if analysis.get('quote_token_address') and len(quote_symbol) > 10:  # Likely an address, not symbol\n",
    "            cache_key = f\"{analysis['quote_token_address']}_{NETWORK}\"\n",
    "            if cache_key in gateway_ds.token_cache:\n",
    "                cached_info = gateway_ds.token_cache[cache_key]\n",
    "                if cached_info and 'symbol' in cached_info:\n",
    "                    quote_symbol = cached_info['symbol']\n",
    "        \n",
    "        # Create token pair name with fee tier and bin step\n",
    "        token_pair = f\"{base_symbol}/{quote_symbol}\"\n",
    "        fee_tier = f\"{analysis['fee_pct']:.2f}%\"\n",
    "        bin_step = analysis['bin_step']\n",
    "        \n",
    "        # Pool header with token pair, fee tier, and bin step\n",
    "        report_message += f\"\"\"ğŸ“Š <b>{token_pair} â€¢ {fee_tier} â€¢ Bin:{bin_step}</b>\n",
    "<i>Pool: {analysis['pool_name']}</i>\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\"\"\"\n",
    "        \n",
    "        # Price and liquidity info with smart formatting using GatewayDataSource\n",
    "        price = analysis['current_price']\n",
    "        price_str = gateway_ds.format_price(price)\n",
    "        report_message += f\"\"\"ğŸ’° <b>Price & Liquidity</b>\n",
    "â€¢ Current Price: <code>{price_str}</code>\n",
    "â€¢ Total Liquidity: <code>{gateway_ds.format_number(analysis['total_liquidity_usd'], is_currency=True)}</code>\n",
    "â€¢ {base_symbol}: <code>{gateway_ds.format_number(analysis['base_liquidity'], decimals=2)}</code>\n",
    "â€¢ {quote_symbol}: <code>{gateway_ds.format_number(analysis['quote_liquidity'], decimals=2)}</code>\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        # Pool parameters\n",
    "        report_message += f\"\"\"âš™ï¸ <b>Pool Parameters</b>\n",
    "â€¢ Fee: <code>{gateway_ds.format_percentage(analysis['fee_pct'])}</code>\n",
    "â€¢ Bin Step: <code>{analysis['bin_step']}</code>\n",
    "â€¢ Active Bin: <code>{analysis['active_bin_id']}</code>\n",
    "â€¢ Total Bins: <code>{analysis['num_bins']}</code>\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        # Liquidity distribution with smart price formatting\n",
    "        min_price_str = gateway_ds.format_price(analysis['price_range']['min'])\n",
    "        max_price_str = gateway_ds.format_price(analysis['price_range']['max'])\n",
    "        report_message += f\"\"\"ğŸ“ˆ <b>Liquidity Distribution</b>\n",
    "â€¢ Price Range: <code>{min_price_str} - {max_price_str}</code>\n",
    "\"\"\"\n",
    "        \n",
    "        # Show liquidity at different ranges instead of key levels\n",
    "        if analysis.get('liquidity_distribution'):\n",
    "            report_message += f\"\\nğŸ¯ <b>Liquidity Around Price</b>\\n\"\n",
    "            for pct in [5, 10, 15, 20]:\n",
    "                if pct in analysis['liquidity_distribution']:\n",
    "                    liquidity_pct = analysis['liquidity_distribution'][pct]\n",
    "                    report_message += f\"â€¢ Â±{pct}%: <code>{liquidity_pct:.1f}%</code> of total liquidity\\n\"\n",
    "        \n",
    "        report_message += \"\\n\"\n",
    "    \n",
    "    # Add footer\n",
    "    report_message += f\"\"\"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "ğŸ’¡ <i>Data from Gateway Meteora CLMM | QuantsLab GatewayDataSource</i>\"\"\"\n",
    "    \n",
    "    return report_message\n",
    "\n",
    "def create_liquidity_chart(analysis: Dict) -> go.Figure:\n",
    "    \"\"\"Create liquidity distribution chart for a pool using GatewayDataSource\"\"\"\n",
    "    \n",
    "    if 'bins_df' not in analysis or analysis['bins_df'].empty:\n",
    "        return None\n",
    "    \n",
    "    bins_df = analysis['bins_df']\n",
    "    \n",
    "    # Use GatewayDataSource chart creation with enhanced title\n",
    "    base_token = analysis.get('base_token', 'BASE')\n",
    "    quote_token = analysis.get('quote_token', 'QUOTE')\n",
    "    fee_tier = f\"{analysis['fee_pct']:.2f}%\"\n",
    "    bin_step = analysis['bin_step']\n",
    "    \n",
    "    title = f\"{base_token}/{quote_token} â€¢ {fee_tier} Fee â€¢ Bin Step: {bin_step}\"\n",
    "    \n",
    "    fig = gateway_ds.create_liquidity_chart(\n",
    "        bins_data=bins_df,\n",
    "        price_col='price',\n",
    "        liquidity_col='total_value',\n",
    "        bin_id_col='binId',\n",
    "        current_price=analysis['current_price'],\n",
    "        active_bin_id=analysis['active_bin_id'],\n",
    "        title=title\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "print(f\"ğŸ“Š Configuration loaded:\")\n",
    "print(f\"  - Gateway URL: {GATEWAY_URL}\")\n",
    "print(f\"  - Network: {NETWORK}\")\n",
    "print(f\"  - Update Interval: {UPDATE_INTERVAL_SECONDS}s\")\n",
    "print(f\"  - Pool Addresses: {len(POOL_ADDRESSES)} configured\")\n",
    "for i, addr in enumerate(POOL_ADDRESSES, 1):\n",
    "    print(f\"    {i}. {addr}\")\n",
    "print(f\"ğŸ”” Enabled notifiers: {', '.join(enabled_notifiers) if enabled_notifiers else 'None configured'}\")\n",
    "print(f\"âœ… GatewayDataSource initialized with Gateway HTTP Client\")\n",
    "print(\"âœ… Meteora Pool Analyzer initialized with GatewayDataSource\")\n",
    "print(\"ğŸ“¤ Telegram reports will be collected and sent at the end\")\n",
    "print(\"ğŸ—‚ï¸ Token cache initialized for efficient lookups\")\n",
    "print(f\"ğŸ“ˆ Cache stats: {gateway_ds.get_cache_stats()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“¡ Fetch Pool Data and Token Metadata from Gateway\n",
    "print(f\"ğŸ”„ Fetching pool data from Gateway...\")\n",
    "print(f\"ğŸ“ Gateway URL: {GATEWAY_URL}\")\n",
    "print(f\"ğŸ”— Network: {NETWORK}\")\n",
    "print(f\"ğŸŠ Pools to analyze: {len(POOL_ADDRESSES)}\\n\")\n",
    "\n",
    "# Test gateway connection using GatewayDataSource\n",
    "try:\n",
    "    is_online = await gateway_ds.ping_gateway()\n",
    "    if is_online:\n",
    "        print(\"âœ… Gateway connection successful\\n\")\n",
    "    else:\n",
    "        print(\"âš ï¸ Gateway is offline, trying to connect...\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Gateway connection error: {e}\\n\")\n",
    "    is_online = False\n",
    "\n",
    "# Fetch all pool data if online\n",
    "all_pool_analyses = []\n",
    "if is_online:\n",
    "    print(\"ğŸ”„ Fetching data for all pools with token metadata...\")\n",
    "    \n",
    "    for i, pool_address in enumerate(POOL_ADDRESSES, 1):\n",
    "        print(f\"\\nğŸ“Š Pool {i}/{len(POOL_ADDRESSES)}: {pool_address}\")\n",
    "        \n",
    "        try:\n",
    "            # Fetch pool data\n",
    "            pool_data = await pool_analyzer.fetch_pool_info(pool_address, NETWORK)\n",
    "            \n",
    "            if pool_data:\n",
    "                print(f\"âœ… Raw pool data fetched\")\n",
    "                \n",
    "                # Display raw pool info\n",
    "                print(f\"ğŸŠ RAW POOL DATA:\")\n",
    "                print(f\"  Pool Address: {pool_data['address']}\")\n",
    "                print(f\"  Base Token Address: {pool_data['baseTokenAddress']}\")\n",
    "                print(f\"  Quote Token Address: {pool_data['quoteTokenAddress']}\")\n",
    "                print(f\"  Current Price: {gateway_ds.format_price(pool_data['price'])}\")\n",
    "                print(f\"  Fee: {gateway_ds.format_percentage(pool_data['feePct'])}\")\n",
    "                print(f\"  Bin Step: {pool_data['binStep']}\")\n",
    "                print(f\"  Active Bin ID: {pool_data['activeBinId']}\")\n",
    "                print(f\"  Total Bins: {len(pool_data.get('bins', []))}\")\n",
    "                \n",
    "                # Fetch token metadata using GatewayDataSource\n",
    "                print(f\"ğŸ” Looking up token metadata...\")\n",
    "                base_token_info = None\n",
    "                quote_token_info = None\n",
    "                base_symbol = pool_data['baseTokenAddress']  # Use address as fallback\n",
    "                quote_symbol = pool_data['quoteTokenAddress']  # Use address as fallback\n",
    "                \n",
    "                # Lookup base token\n",
    "                if pool_data.get('baseTokenAddress'):\n",
    "                    try:\n",
    "                        base_token_info = await pool_analyzer.fetch_token_info(pool_data['baseTokenAddress'], NETWORK)\n",
    "                        if base_token_info and 'symbol' in base_token_info:\n",
    "                            base_symbol = base_token_info['symbol']\n",
    "                            print(f\"  âœ… Base Token: {base_token_info['name']} ({base_token_info['symbol']})\")\n",
    "                            print(f\"    Address: {base_token_info['address']}\")\n",
    "                            print(f\"    Decimals: {base_token_info['decimals']}\")\n",
    "                        else:\n",
    "                            print(f\"  âš ï¸ Base Token: Could not fetch metadata\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"  âŒ Base Token lookup error: {e}\")\n",
    "                \n",
    "                # Lookup quote token  \n",
    "                if pool_data.get('quoteTokenAddress'):\n",
    "                    try:\n",
    "                        quote_token_info = await pool_analyzer.fetch_token_info(pool_data['quoteTokenAddress'], NETWORK)\n",
    "                        if quote_token_info and 'symbol' in quote_token_info:\n",
    "                            quote_symbol = quote_token_info['symbol']\n",
    "                            print(f\"  âœ… Quote Token: {quote_token_info['name']} ({quote_token_info['symbol']})\")\n",
    "                            print(f\"    Address: {quote_token_info['address']}\")\n",
    "                            print(f\"    Decimals: {quote_token_info['decimals']}\")\n",
    "                        else:\n",
    "                            print(f\"  âš ï¸ Quote Token: Could not fetch metadata\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"  âŒ Quote Token lookup error: {e}\")\n",
    "                \n",
    "                # Create token pair name\n",
    "                token_pair = f\"{base_symbol}/{quote_symbol}\"\n",
    "                pool_name = f\"{token_pair} Pool\"\n",
    "                \n",
    "                print(f\"ğŸ·ï¸ Identified Pool: {token_pair}\")\n",
    "                \n",
    "                # Create metadata with token info\n",
    "                metadata = {\n",
    "                    'pool_name': pool_name,\n",
    "                    'token_pair': token_pair,\n",
    "                    'base_token_symbol': base_symbol,\n",
    "                    'quote_token_symbol': quote_symbol,\n",
    "                    'base_token_info': base_token_info,\n",
    "                    'quote_token_info': quote_token_info\n",
    "                }\n",
    "                \n",
    "                # Perform analysis\n",
    "                analysis = pool_analyzer.analyze_pool(pool_data, metadata)\n",
    "                all_pool_analyses.append(analysis)\n",
    "                \n",
    "                print(f\"ğŸ“Š Analysis Summary:\")\n",
    "                print(f\"  Price: {gateway_ds.format_price(analysis['current_price'])}\")\n",
    "                print(f\"  Liquidity: {gateway_ds.format_number(analysis['total_liquidity_usd'], is_currency=True)}\")\n",
    "                print(f\"  Base ({base_symbol}): {gateway_ds.format_number(analysis['base_liquidity'], decimals=2)}\")\n",
    "                print(f\"  Quote ({quote_symbol}): {gateway_ds.format_number(analysis['quote_liquidity'], decimals=2)}\")\n",
    "                print(f\"  Bins: {analysis['num_bins']}\")\n",
    "                \n",
    "            else:\n",
    "                print(f\"âŒ Failed to fetch data for pool {pool_address}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error analyzing pool {pool_address}: {e}\")\n",
    "    \n",
    "    print(f\"\\nâœ… Successfully analyzed {len(all_pool_analyses)} out of {len(POOL_ADDRESSES)} pools\")\n",
    "    \n",
    "    if all_pool_analyses:\n",
    "        total_liquidity = sum(a['total_liquidity_usd'] for a in all_pool_analyses)\n",
    "        print(f\"ğŸ’° Total Liquidity Across All Pools: {gateway_ds.format_number(total_liquidity, is_currency=True)}\")\n",
    "        \n",
    "        # Display summary with token pairs using GatewayDataSource formatting\n",
    "        print(f\"\\nğŸ“Š POOL ANALYSIS SUMMARY:\")\n",
    "        print(\"=\" * 80)\n",
    "        for analysis in all_pool_analyses:\n",
    "            token_pair = analysis.get('token_pair', analysis['pool_name'])\n",
    "            print(f\"{token_pair:<20} | \"\n",
    "                  f\"{gateway_ds.format_price(analysis['current_price']):<12} | \"\n",
    "                  f\"{gateway_ds.format_number(analysis['total_liquidity_usd'], is_currency=True):<12} | \"\n",
    "                  f\"{analysis['num_bins']:>4} bins\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Display cache statistics\n",
    "        cache_stats = gateway_ds.get_cache_stats()\n",
    "        print(f\"\\nğŸ—‚ï¸ Token Cache Stats: {cache_stats['cached_tokens']} tokens cached, {cache_stats['failed_lookups']} failed lookups\")\n",
    "        \n",
    "else:\n",
    "    print(\"âŒ Cannot fetch pool data - Gateway is offline\")\n",
    "    all_pool_analyses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸš€ Generate Pool Analysis Report with Token Lookup (Preview Only)\n",
    "if all_pool_analyses:    \n",
    "    # Generate report with token symbol lookup using GatewayDataSource\n",
    "    report_message = await generate_pool_report(all_pool_analyses)\n",
    "    \n",
    "    # Store the report for sending later\n",
    "    telegram_reports['report_message'] = report_message\n",
    "    \n",
    "    print(\"ğŸ“¤ Pool analysis report generated successfully\")\n",
    "    print(f\"ğŸ“Š Report includes {len(all_pool_analyses)} pool(s) with token symbols\")\n",
    "    print(\"ğŸ“‹ Report stored for Telegram sending at the end\\n\")\n",
    "    \n",
    "    # Display detailed analysis for each pool using GatewayDataSource formatting\n",
    "    print(\"ğŸ“Š DETAILED POOL ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    for i, analysis in enumerate(all_pool_analyses, 1):\n",
    "        print(f\"\\nğŸ“Š POOL {i} - {analysis['pool_name']}\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Address: {analysis['address']}\")\n",
    "        print(f\"Current Price: {gateway_ds.format_price(analysis['current_price'])}\")\n",
    "        print(f\"Total Liquidity (USD): {gateway_ds.format_number(analysis['total_liquidity_usd'], is_currency=True)}\")\n",
    "        print(f\"Base Liquidity: {gateway_ds.format_number(analysis['base_liquidity'], decimals=2)} {analysis['base_token']}\")\n",
    "        print(f\"Quote Liquidity: {gateway_ds.format_number(analysis['quote_liquidity'], decimals=2)} {analysis['quote_token']}\")\n",
    "        print(f\"Liquidity Concentration: {gateway_ds.format_percentage(analysis['liquidity_concentration'], decimals=1)} (within Â±5 bins)\")\n",
    "        print(f\"Number of Bins: {analysis['num_bins']}\")\n",
    "        print(f\"Price Range: {gateway_ds.format_price(analysis['price_range']['min'])} - {gateway_ds.format_price(analysis['price_range']['max'])}\")\n",
    "        print(f\"Fee: {gateway_ds.format_percentage(analysis['fee_pct'])}\")\n",
    "        print(f\"Bin Step: {analysis['bin_step']}\")\n",
    "        print(f\"Active Bin ID: {analysis['active_bin_id']}\")\n",
    "        \n",
    "        if analysis.get('key_levels'):\n",
    "            print(f\"\\nğŸ¯ KEY LIQUIDITY LEVELS:\")\n",
    "            for j, level in enumerate(analysis['key_levels'], 1):\n",
    "                print(f\"  {j}. Price: {gateway_ds.format_price(level['price'])} | Liquidity: {gateway_ds.format_number(level['total_value'], is_currency=True)}\")\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "    \n",
    "    print(\"\\nğŸ“‹ TELEGRAM REPORT PREVIEW:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(report_message.replace('<b>', '**').replace('</b>', '**')\n",
    "          .replace('<code>', '`').replace('</code>', '`')\n",
    "          .replace('<i>', '_').replace('</i>', '_'))\n",
    "    print(\"=\" * 50)\n",
    "        \n",
    "else:\n",
    "    report_message = await generate_pool_report([])\n",
    "    print(\"âš ï¸ No pool data available for report generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¨ Generate Liquidity Distribution Charts with Token Symbols (Save for Later)\n",
    "if all_pool_analyses:\n",
    "    print(\"ğŸ“Š Generating liquidity distribution charts...\\n\")\n",
    "    \n",
    "    # Clear any existing chart files from previous runs\n",
    "    telegram_reports['chart_files'] = []\n",
    "    \n",
    "    for i, analysis in enumerate(all_pool_analyses, 1):\n",
    "        print(f\"ğŸ¨ Chart {i}/{len(all_pool_analyses)}: {analysis['pool_name']}\")\n",
    "        \n",
    "        # Use cached token symbols from initial pool fetching\n",
    "        base_symbol = analysis['base_token']\n",
    "        quote_symbol = analysis['quote_token']\n",
    "        \n",
    "        # Use GatewayDataSource token cache if available and token symbols are still addresses\n",
    "        if analysis.get('base_token_address') and len(base_symbol) > 10:\n",
    "            cache_key = f\"{analysis['base_token_address']}_{NETWORK}\"\n",
    "            if cache_key in gateway_ds.token_cache:\n",
    "                cached_info = gateway_ds.token_cache[cache_key]\n",
    "                if cached_info and 'symbol' in cached_info:\n",
    "                    base_symbol = cached_info['symbol']\n",
    "                    \n",
    "        if analysis.get('quote_token_address') and len(quote_symbol) > 10:\n",
    "            cache_key = f\"{analysis['quote_token_address']}_{NETWORK}\"\n",
    "            if cache_key in gateway_ds.token_cache:\n",
    "                cached_info = gateway_ds.token_cache[cache_key]\n",
    "                if cached_info and 'symbol' in cached_info:\n",
    "                    quote_symbol = cached_info['symbol']\n",
    "        \n",
    "        # Update analysis with proper symbols for chart generation\n",
    "        analysis_with_symbols = analysis.copy()\n",
    "        analysis_with_symbols['base_token'] = base_symbol\n",
    "        analysis_with_symbols['quote_token'] = quote_symbol\n",
    "        analysis_with_symbols['token_pair'] = f\"{base_symbol}/{quote_symbol}\"\n",
    "        \n",
    "        # Create chart for this pool with proper token symbols using GatewayDataSource\n",
    "        fig = create_liquidity_chart(analysis_with_symbols)\n",
    "        \n",
    "        if fig:\n",
    "            print(f\"  âœ… Chart generated for {analysis_with_symbols['token_pair']}\")\n",
    "            fig.show()\n",
    "            \n",
    "            # Save chart as PNG with token pair, fee tier, and bin step in filename\n",
    "            safe_pair_name = analysis_with_symbols['token_pair'].replace('/', '_')\n",
    "            fee_tier = f\"{analysis['fee_pct']:.2f}\".replace('.', 'p')  # 0.04 -> 0p04\n",
    "            bin_step = analysis['bin_step']\n",
    "            chart_filename = f\"/tmp/meteora_{safe_pair_name}_{fee_tier}pct_bin{bin_step}_chart.png\"\n",
    "            \n",
    "            # Use GatewayDataSource save_chart method\n",
    "            if gateway_ds.save_chart(fig, chart_filename, format=\"png\"):\n",
    "                print(f\"  ğŸ’¾ Chart saved using GatewayDataSource: {chart_filename}\")\n",
    "                \n",
    "                # Store chart info for Telegram sending later with enhanced caption\n",
    "                chart_info = {\n",
    "                    'filename': chart_filename,\n",
    "                    'pool_name': analysis['pool_name'],\n",
    "                    'token_pair': analysis_with_symbols['token_pair'],\n",
    "                    'caption': f\"\"\"ğŸ“Š <b>Liquidity Distribution Chart</b>\n",
    "ğŸŠ Pool: {analysis_with_symbols['token_pair']} â€¢ {gateway_ds.format_percentage(analysis['fee_pct'])} Fee â€¢ Bin:{analysis['bin_step']}\n",
    "ğŸ’° Total Liquidity: {gateway_ds.format_number(analysis['total_liquidity_usd'], is_currency=True)}\n",
    "ğŸ“ Current Price: {gateway_ds.format_price(analysis['current_price'])}\n",
    "ğŸ¯ Concentration: {gateway_ds.format_percentage(analysis['liquidity_concentration'], decimals=1)} near price\n",
    "âš™ï¸ Active Bin: {analysis['active_bin_id']} | Total Bins: {analysis['num_bins']}\n",
    "\"\"\"\n",
    "                }\n",
    "                telegram_reports['chart_files'].append(chart_info)\n",
    "                print(f\"  ğŸ“‹ Chart stored for Telegram sending\")\n",
    "                        \n",
    "            else:\n",
    "                print(f\"  âŒ Error saving chart with GatewayDataSource\")\n",
    "        else:\n",
    "            print(f\"  âš ï¸ No bin data available for {analysis['pool_name']}\")\n",
    "            \n",
    "        print()  # Empty line between pools\n",
    "        \n",
    "    print(f\"âœ… Chart generation completed for {len(all_pool_analyses)} pool(s)\")\n",
    "    print(f\"ğŸ“‹ {len(telegram_reports['chart_files'])} charts stored for Telegram sending\")\n",
    "else:\n",
    "    print(\"âš ï¸ No pool analyses available for chart generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š Generate CSV Reports with Token Symbols (Save for Later)\n",
    "if all_pool_analyses:\n",
    "    print(\"ğŸ“‹ Generating detailed CSV reports...\\n\")\n",
    "    \n",
    "    # Clear any existing CSV files from previous runs\n",
    "    telegram_reports['csv_files'] = []\n",
    "    \n",
    "    for i, analysis in enumerate(all_pool_analyses, 1):\n",
    "        if 'bins_df' in analysis and not analysis['bins_df'].empty:\n",
    "            print(f\"ğŸ“„ CSV Report {i}/{len(all_pool_analyses)}: {analysis['pool_name']}\")\n",
    "            \n",
    "            # Use cached token symbols from initial pool fetching\n",
    "            base_symbol = analysis['base_token']\n",
    "            quote_symbol = analysis['quote_token']\n",
    "            \n",
    "            # Use GatewayDataSource token cache if available and token symbols are still addresses\n",
    "            if analysis.get('base_token_address') and len(base_symbol) > 10:\n",
    "                cache_key = f\"{analysis['base_token_address']}_{NETWORK}\"\n",
    "                if cache_key in gateway_ds.token_cache:\n",
    "                    cached_info = gateway_ds.token_cache[cache_key]\n",
    "                    if cached_info and 'symbol' in cached_info:\n",
    "                        base_symbol = cached_info['symbol']\n",
    "                        \n",
    "            if analysis.get('quote_token_address') and len(quote_symbol) > 10:\n",
    "                cache_key = f\"{analysis['quote_token_address']}_{NETWORK}\"\n",
    "                if cache_key in gateway_ds.token_cache:\n",
    "                    cached_info = gateway_ds.token_cache[cache_key]\n",
    "                    if cached_info and 'symbol' in cached_info:\n",
    "                        quote_symbol = cached_info['symbol']\n",
    "            \n",
    "            token_pair = f\"{base_symbol}/{quote_symbol}\"\n",
    "            \n",
    "            # Prepare detailed DataFrame\n",
    "            detailed_df = analysis['bins_df'].copy()\n",
    "            \n",
    "            # Add calculated fields with proper token symbols, fee tier, and bin step\n",
    "            detailed_df['token_pair'] = token_pair\n",
    "            detailed_df['base_token_symbol'] = base_symbol\n",
    "            detailed_df['quote_token_symbol'] = quote_symbol\n",
    "            detailed_df['fee_pct'] = analysis['fee_pct']\n",
    "            detailed_df['bin_step'] = analysis['bin_step']\n",
    "            detailed_df['pool_name'] = analysis['pool_name']\n",
    "            detailed_df['pool_address'] = analysis['address']\n",
    "            detailed_df['base_token_address'] = analysis.get('base_token_address', '')\n",
    "            detailed_df['quote_token_address'] = analysis.get('quote_token_address', '')\n",
    "            detailed_df['current_price'] = analysis['current_price']\n",
    "            detailed_df['is_active_bin'] = detailed_df['binId'] == analysis['active_bin_id']\n",
    "            detailed_df['distance_from_price'] = ((detailed_df['price'] - analysis['current_price']) / analysis['current_price'] * 100)\n",
    "            detailed_df['total_value_usd'] = (\n",
    "                detailed_df['baseTokenAmount'] * analysis['current_price'] + \n",
    "                detailed_df['quoteTokenAmount']\n",
    "            )\n",
    "            detailed_df['liquidity_share_pct'] = (detailed_df['total_value_usd'] / detailed_df['total_value_usd'].sum() * 100)\n",
    "            \n",
    "            # Reorder columns with token information, fee tier, and bin step first\n",
    "            column_order = [\n",
    "                'token_pair', 'base_token_symbol', 'quote_token_symbol', 'fee_pct', 'bin_step',\n",
    "                'pool_name', 'pool_address', 'base_token_address', 'quote_token_address',\n",
    "                'binId', 'price', 'is_active_bin',\n",
    "                'baseTokenAmount', 'quoteTokenAmount', 'total_value_usd',\n",
    "                'liquidity_share_pct', 'distance_from_price', 'current_price'\n",
    "            ]\n",
    "            \n",
    "            detailed_df = detailed_df[column_order]\n",
    "            \n",
    "            # Create CSV filename with token pair, fee tier, and bin step\n",
    "            timestamp_str = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M\")\n",
    "            safe_pair_name = token_pair.replace('/', '_')\n",
    "            fee_tier = f\"{analysis['fee_pct']:.2f}\".replace('.', 'p')  # 0.04 -> 0p04\n",
    "            bin_step = analysis['bin_step']\n",
    "            csv_filename = f\"/tmp/meteora_{safe_pair_name}_{fee_tier}pct_bin{bin_step}_{timestamp_str}.csv\"\n",
    "            \n",
    "            # Create metadata for GatewayDataSource export\n",
    "            csv_metadata = {\n",
    "                'token_pair': token_pair,\n",
    "                'pool_address': analysis['address'],\n",
    "                'network': NETWORK,\n",
    "                'analysis_timestamp': gateway_ds.format_timestamp(analysis['timestamp'])\n",
    "            }\n",
    "            \n",
    "            # Use GatewayDataSource export_to_csv method\n",
    "            if gateway_ds.export_to_csv(detailed_df, csv_filename, metadata=csv_metadata):\n",
    "                print(f\"  âœ… CSV exported using GatewayDataSource: {csv_filename}\")\n",
    "                print(f\"  ğŸ“Š Records: {len(detailed_df)}\")\n",
    "                \n",
    "                # Display sample data with token symbols, fee tier, and bin step using GatewayDataSource formatting\n",
    "                print(f\"  ğŸ“‹ CSV Preview (top 3 rows):\")\n",
    "                display_df = detailed_df.head(3)[['token_pair', 'fee_pct', 'bin_step', 'binId', 'price', 'total_value_usd', 'liquidity_share_pct']]\n",
    "                for _, row in display_df.iterrows():\n",
    "                    print(f\"    {row['token_pair']} {gateway_ds.format_percentage(row['fee_pct'])} Bin{row['bin_step']} | \"\n",
    "                          f\"Bin {row['binId']}: {gateway_ds.format_price(row['price'])} | \"\n",
    "                          f\"{gateway_ds.format_number(row['total_value_usd'], is_currency=True)} \"\n",
    "                          f\"({gateway_ds.format_percentage(row['liquidity_share_pct'])})\")\n",
    "                \n",
    "                # Store CSV info for Telegram sending later with enhanced caption\n",
    "                csv_info = {\n",
    "                    'filename': csv_filename,\n",
    "                    'pool_name': analysis['pool_name'],\n",
    "                    'token_pair': token_pair,\n",
    "                    'caption': f\"\"\"ğŸ“‹ <b>Detailed Pool Data Export</b>\n",
    "ğŸŠ Pool: {token_pair} â€¢ {gateway_ds.format_percentage(analysis['fee_pct'])} Fee â€¢ Bin:{analysis['bin_step']}\n",
    "ğŸ“… {gateway_ds.format_timestamp(datetime.now(timezone.utc))}\n",
    "\n",
    "ğŸ“Š Dataset Summary:\n",
    "â€¢ Total Bins: {len(detailed_df)}\n",
    "â€¢ Active Bin: {analysis['active_bin_id']}\n",
    "â€¢ Price Range: {gateway_ds.format_price(detailed_df['price'].min())} - {gateway_ds.format_price(detailed_df['price'].max())}\n",
    "â€¢ Total Liquidity: {gateway_ds.format_number(analysis['total_liquidity_usd'], is_currency=True)}\n",
    "â€¢ Fee Tier: {gateway_ds.format_percentage(analysis['fee_pct'])} | Bin Step: {analysis['bin_step']}\n",
    "\"\"\"\n",
    "                }\n",
    "                telegram_reports['csv_files'].append(csv_info)\n",
    "                print(f\"  ğŸ“‹ CSV stored for Telegram sending\")\n",
    "                \n",
    "            else:\n",
    "                print(f\"  âŒ Error exporting CSV with GatewayDataSource\")\n",
    "            \n",
    "            print()  # Empty line between pools\n",
    "            \n",
    "        else:\n",
    "            print(f\"  âš ï¸ No bin data available for {analysis['pool_name']}\")\n",
    "    \n",
    "    print(f\"âœ… CSV report generation completed for all pools\")\n",
    "    print(f\"ğŸ“‹ {len(telegram_reports['csv_files'])} CSV files stored for Telegram sending\")\n",
    "else:\n",
    "    print(\"âš ï¸ No pool analyses available for CSV export\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“¤ Send All Reports to Telegram\n",
    "print(\"ğŸš€ SENDING ALL REPORTS TO TELEGRAM\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "telegram_notifier = notification_manager.get_notifier('telegram')\n",
    "if not telegram_notifier:\n",
    "    print(\"âŒ Telegram notifier not configured\")\n",
    "    print(\"ğŸ’¡ Configure Telegram in .env to send reports\")\n",
    "else:\n",
    "    # Summary of what will be sent\n",
    "    text_count = 1 if telegram_reports['report_message'] else 0\n",
    "    chart_count = len(telegram_reports['chart_files'])\n",
    "    csv_count = len(telegram_reports['csv_files'])\n",
    "    total_items = text_count + chart_count + csv_count\n",
    "    \n",
    "    print(f\"ğŸ“Š Sending {total_items} items to Telegram:\")\n",
    "    print(f\"  â€¢ {text_count} text report\")\n",
    "    print(f\"  â€¢ {chart_count} chart files\")\n",
    "    print(f\"  â€¢ {csv_count} CSV files\")\n",
    "    print()\n",
    "    \n",
    "    sent_count = 0\n",
    "    failed_count = 0\n",
    "    \n",
    "    # Send text report first\n",
    "    if telegram_reports['report_message']:\n",
    "        print(\"1ï¸âƒ£ Sending text report...\")\n",
    "        try:\n",
    "            notification_msg = NotificationMessage(\n",
    "                title=\"Meteora Pool Analysis\",\n",
    "                message=telegram_reports['report_message'],\n",
    "                level=\"info\"\n",
    "            )\n",
    "            \n",
    "            if await telegram_notifier.send_notification(notification_msg):\n",
    "                print(\"  âœ… Text report sent successfully\")\n",
    "                sent_count += 1\n",
    "            else:\n",
    "                print(\"  âŒ Failed to send text report\")\n",
    "                failed_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ Error sending text report: {e}\")\n",
    "            failed_count += 1\n",
    "        print()\n",
    "    \n",
    "    # Send chart files\n",
    "    if telegram_reports['chart_files']:\n",
    "        print(\"2ï¸âƒ£ Sending chart files...\")\n",
    "        for i, chart_info in enumerate(telegram_reports['chart_files'], 1):\n",
    "            print(f\"  ğŸ“Š Chart {i}/{chart_count}: {chart_info['pool_name']}\")\n",
    "            try:\n",
    "                if await telegram_notifier.send_photo(chart_info['filename'], chart_info['caption']):\n",
    "                    print(f\"    âœ… Chart sent successfully\")\n",
    "                    sent_count += 1\n",
    "                else:\n",
    "                    print(f\"    âŒ Failed to send chart\")\n",
    "                    failed_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"    âŒ Error sending chart: {e}\")\n",
    "                failed_count += 1\n",
    "        print()\n",
    "    \n",
    "    # Send CSV files\n",
    "    if telegram_reports['csv_files']:\n",
    "        print(\"3ï¸âƒ£ Sending CSV files...\")\n",
    "        for i, csv_info in enumerate(telegram_reports['csv_files'], 1):\n",
    "            print(f\"  ğŸ“„ CSV {i}/{csv_count}: {csv_info['pool_name']}\")\n",
    "            try:\n",
    "                if await telegram_notifier.send_document(csv_info['filename'], csv_info['caption']):\n",
    "                    print(f\"    âœ… CSV sent successfully\")\n",
    "                    sent_count += 1\n",
    "                else:\n",
    "                    print(f\"    âŒ Failed to send CSV\")\n",
    "                    failed_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"    âŒ Error sending CSV: {e}\")\n",
    "                failed_count += 1\n",
    "        print()\n",
    "    \n",
    "    # Final summary\n",
    "    print(\"ğŸ“Š TELEGRAM SENDING SUMMARY\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"âœ… Successfully sent: {sent_count}/{total_items}\")\n",
    "    print(f\"âŒ Failed to send: {failed_count}/{total_items}\")\n",
    "    \n",
    "    if sent_count > 0:\n",
    "        print(f\"\\nğŸ‰ Check your Telegram for {sent_count} new messages!\")\n",
    "    \n",
    "    if failed_count > 0:\n",
    "        print(f\"\\nâš ï¸ {failed_count} items failed to send - check error messages above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”’ Cleanup and Execution Summary\n",
    "print(\"âœ… Meteora pool analysis completed successfully\\n\")\n",
    "\n",
    "# Clean up temporary files (optional)\n",
    "import os\n",
    "import glob\n",
    "\n",
    "temp_files = glob.glob('/tmp/meteora_*.png') + glob.glob('/tmp/meteora_*.html') + glob.glob('/tmp/meteora_*.csv')\n",
    "cleanup_count = 0\n",
    "\n",
    "for file_path in temp_files:\n",
    "    try:\n",
    "        # Keep the most recent files\n",
    "        file_age = datetime.now() - datetime.fromtimestamp(os.path.getctime(file_path))\n",
    "        if file_age.days > 0:  # Clean files older than 1 day\n",
    "            os.remove(file_path)\n",
    "            cleanup_count += 1\n",
    "            print(f\"ğŸ—‘ï¸ Cleaned up: {os.path.basename(file_path)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Could not remove {file_path}: {e}\")\n",
    "\n",
    "if cleanup_count > 0:\n",
    "    print(f\"\\nğŸ§¹ Cleaned up {cleanup_count} old temporary files\")\n",
    "\n",
    "# Execution Summary\n",
    "print(\"\\nğŸ“Š EXECUTION SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"ğŸŒ Data Source: Gateway API ({GATEWAY_URL})\")\n",
    "print(f\"ğŸ“¡ Using: QuantsLab GatewayDataSource (Hummingbot Gateway HTTP Client)\")\n",
    "print(f\"ğŸ”— Network: Solana {NETWORK}\")\n",
    "\n",
    "if 'all_pool_analyses' in locals() and all_pool_analyses:\n",
    "    total_liquidity = sum(a['total_liquidity_usd'] for a in all_pool_analyses)\n",
    "    print(f\"ğŸŠ Pools Analyzed: {len(all_pool_analyses)}\")\n",
    "    print(f\"ğŸ’° Total Liquidity: {gateway_ds.format_number(total_liquidity, is_currency=True)}\")\n",
    "    print(f\"ğŸ“¤ Reports Sent: {'âœ… Yes' if enabled_notifiers else 'âŒ No notifiers configured'}\")\n",
    "    print(f\"ğŸ“Š Charts Generated: âœ… Yes (via GatewayDataSource)\")\n",
    "    print(f\"ğŸ“‹ CSV Exports: âœ… Yes (via GatewayDataSource)\")\n",
    "    \n",
    "    # Display final cache statistics\n",
    "    cache_stats = gateway_ds.get_cache_stats()\n",
    "    print(f\"ğŸ—‚ï¸ Final Cache Stats:\")\n",
    "    print(f\"  â€¢ Token lookups cached: {cache_stats['cached_tokens']}\")\n",
    "    print(f\"  â€¢ Failed token lookups: {cache_stats['failed_lookups']}\")\n",
    "    print(f\"  â€¢ Total cache entries: {cache_stats['token_cache_size']}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"âš ï¸ No pools analyzed - check Gateway connection\")\n",
    "\n",
    "print(\"\\nğŸ‰ Meteora pool analysis complete!\")\n",
    "print(\"ğŸ”§ Powered by QuantsLab GatewayDataSource for standardized Gateway interactions\")\n",
    "\n",
    "if 'telegram' in enabled_notifiers:\n",
    "    print(\"ğŸ“± Check your Telegram for delivered reports, charts, and data files\")\n",
    "else:\n",
    "    print(\"ğŸ’¡ Configure Telegram in .env to receive automated reports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quants-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
