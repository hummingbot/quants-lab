{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# This is necessary to recognize the modules\n",
    "import os\n",
    "import sys\n",
    "from decimal import Decimal\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "root_path = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
    "sys.path.append(root_path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "from core.data_sources.clob import CLOBDataSource\n",
    "\n",
    "# Get trading rules and candles\n",
    "clob = CLOBDataSource()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "clob.load_candles_cache(root_path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "candles = clob.candles_cache[(\"binance\", \"BTC-USDT\", \"1s\")]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df = candles.data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from core.backtesting.triple_barrier_method import triple_barrier_method\n",
    "df[\"side\"] = 1\n",
    "df_with_tbm = triple_barrier_method(df, tp=3.5, sl=3.5, tl=300, std_span=200, trade_cost=0.0000)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_with_tbm.close_type.value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_with_tbm.target.describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Add technical indicators using pandas_ta\n",
    "import pandas_ta as ta\n",
    "\n",
    "# Create a copy to work with\n",
    "df_with_indicators = df_with_tbm.copy()\n",
    "\n",
    "# Bollinger Bands with different lengths\n",
    "df_with_indicators.ta.bbands(length=20, std=2, append=True)  # Standard BB\n",
    "df_with_indicators.ta.bbands(length=50, std=2, append=True)  # Longer term BB\n",
    "\n",
    "# MACD with different parameters\n",
    "df_with_indicators.ta.macd(fast=12, slow=26, signal=9, append=True)  # Standard MACD\n",
    "df_with_indicators.ta.macd(fast=8, slow=21, signal=5, append=True)  # Faster MACD\n",
    "\n",
    "# RSI with different lengths\n",
    "df_with_indicators.ta.rsi(length=14, append=True)  # Standard RSI\n",
    "df_with_indicators.ta.rsi(length=21, append=True)  # Longer RSI\n",
    "\n",
    "# Moving averages\n",
    "df_with_indicators.ta.sma(length=20, append=True)  # Short MA\n",
    "df_with_indicators.ta.sma(length=50, append=True)  # Medium MA\n",
    "df_with_indicators.ta.ema(length=20, append=True)  # Short EMA\n",
    "df_with_indicators.ta.ema(length=50, append=True)  # Medium EMA\n",
    "\n",
    "# Volatility and momentum indicators\n",
    "df_with_indicators.ta.atr(length=14, append=True)  # ATR\n",
    "df_with_indicators.ta.stoch(k=14, d=3, append=True)  # Stochastic\n",
    "df_with_indicators.ta.adx(length=14, append=True)  # ADX\n",
    "\n",
    "# Replace df_with_tbm with df_with_indicators for further processing\n",
    "df_processed = df_with_indicators.copy()\n",
    "\n",
    "# df_processed.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# 1. Remove unnecessary columns\n",
    "columns_to_drop = ['timestamp', 'taker_buy_base_volume', 'volume', \n",
    "                   'close_time', 'real_class', 'ret', 'tp', 'sl', 'take_profit_time', 'stop_loss_time', 'tl', 'side']\n",
    "df_processed = df_processed.drop(columns=columns_to_drop)\n",
    "# 2. Convert prices to returns\n",
    "price_columns = ['open', 'high', 'low', 'close']\n",
    "for col in price_columns:\n",
    "    df_processed[f'{col}_ret'] = df_processed[col].pct_change()\n",
    "df_processed = df_processed.drop(columns=price_columns)\n",
    "\n",
    "# 3. Create buy/sell volume ratio\n",
    "df_processed['buy_volume_ratio'] = df_processed['taker_buy_quote_volume'] / df_processed['quote_asset_volume']\n",
    "df_processed = df_processed.drop(columns=['taker_buy_quote_volume'])\n",
    "\n",
    "# 4. Drop any rows with NaN values (first row will have NaN due to returns calculation)\n",
    "df_processed = df_processed.dropna()\n",
    "\n",
    "# 5. Get all numeric columns for scaling (excluding the target 'close_type')\n",
    "numeric_columns = df_processed.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "numeric_columns.remove('close_type')  # Don't scale the target variable\n",
    "\n",
    "# 6. Apply StandardScaler to all numeric columns\n",
    "scaler = StandardScaler()\n",
    "df_processed[numeric_columns] = scaler.fit_transform(df_processed[numeric_columns])\n",
    "\n",
    "# Show the first few rows of the processed dataset\n",
    "print(\"Processed dataset shape:\", df_processed.shape)\n",
    "df_processed.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "candles_path = os.path.join(root_path, \"data\", \"features_df\")\n",
    "filename = os.path.join(candles_path, f\"{candles.connector_name}|{candles.trading_pair}|{candles.interval}.parquet\")\n",
    "df_processed.to_parquet(\n",
    "filename,\n",
    "engine='pyarrow',\n",
    "compression='snappy',\n",
    "index=True\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# dump the scaler\n",
    "import joblib\n",
    "\n",
    "joblib.dump(scaler, os.path.join(root_path, \"models\", \"scaler.pkl\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
